{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "HItsinp0SFqA",
   "metadata": {
    "id": "HItsinp0SFqA"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb96f0ec-3525-4e79-9ad1-8e95c15a9cf2",
   "metadata": {
    "executionInfo": {
     "elapsed": 4342,
     "status": "ok",
     "timestamp": 1698238165197,
     "user": {
      "displayName": "Khoa Nguyen Tho Anh",
      "userId": "05392028195404260378"
     },
     "user_tz": -420
    },
    "id": "bb96f0ec-3525-4e79-9ad1-8e95c15a9cf2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73f1e96-b8ea-4b8f-b4f3-9b3f27b42acd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6185,
     "status": "ok",
     "timestamp": 1698238171377,
     "user": {
      "displayName": "Khoa Nguyen Tho Anh",
      "userId": "05392028195404260378"
     },
     "user_tz": -420
    },
    "id": "c73f1e96-b8ea-4b8f-b4f3-9b3f27b42acd",
    "outputId": "186ef4b9-cb36-4d94-9795-5ef79d712dc4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:10<00:00, 973kB/s] \n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 188kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.09MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 4.53MB/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_epochs = 10\n",
    "\n",
    "train_dataset = MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "589bb0df",
   "metadata": {
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1698238197296,
     "user": {
      "displayName": "Khoa Nguyen Tho Anh",
      "userId": "05392028195404260378"
     },
     "user_tz": -420
    },
    "id": "589bb0df"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.05)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer3(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        out = self.output(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b8c275",
   "metadata": {},
   "source": [
    "Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db5beca-ee90-4d2e-bb2f-62b20da68cf3",
   "metadata": {
    "executionInfo": {
     "elapsed": 6465,
     "status": "ok",
     "timestamp": 1698238206561,
     "user": {
      "displayName": "Khoa Nguyen Tho Anh",
      "userId": "05392028195404260378"
     },
     "user_tz": -420
    },
    "id": "0db5beca-ee90-4d2e-bb2f-62b20da68cf3"
   },
   "outputs": [],
   "source": [
    "model = MLP(input_dims=784, hidden_dims=256, output_dims=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21211483-aeed-4beb-aeea-d1e58ae7baf9",
   "metadata": {
    "id": "21211483-aeed-4beb-aeea-d1e58ae7baf9",
    "outputId": "f889acbf-73b2-4500-c60f-79c7f00a69cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train_Loss: 2.3407, Train_Acc: 0.0978\n",
      "Epoch 2/10, Train_Loss: 2.3094, Train_Acc: 0.1066\n",
      "Epoch 3/10, Train_Loss: 2.3030, Train_Acc: 0.1124\n",
      "Epoch 4/10, Train_Loss: 2.3016, Train_Acc: 0.1124\n",
      "Epoch 5/10, Train_Loss: 2.3013, Train_Acc: 0.1124\n",
      "Epoch 6/10, Train_Loss: 2.3012, Train_Acc: 0.1124\n",
      "Epoch 7/10, Train_Loss: 2.3012, Train_Acc: 0.1124\n",
      "Epoch 8/10, Train_Loss: 2.3012, Train_Acc: 0.1124\n",
      "Epoch 9/10, Train_Loss: 2.3011, Train_Acc: 0.1124\n",
      "Epoch 10/10, Train_Loss: 2.3011, Train_Acc: 0.1124\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):    \n",
    "    t_loss = 0\n",
    "    t_acc = 0\n",
    "    cnt = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        t_loss += loss.item()\n",
    "        t_acc += (torch.argmax(outputs, 1) == y).sum().item()\n",
    "        cnt += len(y)\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    t_acc /= cnt\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train_Loss: {t_loss:.4f}, Train_Acc: {t_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c78872",
   "metadata": {},
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ca9130",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_dims=784, hidden_dims=256, output_dims=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7b5657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train_Loss: 2.3175, Train_Acc: 0.1377\n",
      "Epoch 2/10, Train_Loss: 1.7754, Train_Acc: 0.4937\n",
      "Epoch 3/10, Train_Loss: 0.8369, Train_Acc: 0.7633\n",
      "Epoch 4/10, Train_Loss: 0.5028, Train_Acc: 0.8691\n",
      "Epoch 5/10, Train_Loss: 0.3605, Train_Acc: 0.9012\n",
      "Epoch 6/10, Train_Loss: 0.2940, Train_Acc: 0.9173\n",
      "Epoch 7/10, Train_Loss: 0.2506, Train_Acc: 0.9283\n",
      "Epoch 8/10, Train_Loss: 0.2181, Train_Acc: 0.9376\n",
      "Epoch 9/10, Train_Loss: 0.1897, Train_Acc: 0.9455\n",
      "Epoch 10/10, Train_Loss: 0.1657, Train_Acc: 0.9530\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    t_loss = 0\n",
    "    t_acc = 0\n",
    "    cnt = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t_loss += loss.item()\n",
    "        t_acc += (torch.argmax(outputs, 1) == y).sum().item()\n",
    "        cnt += len(y)\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    t_acc /= cnt\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train_Loss: {t_loss:.4f}, Train_Acc: {t_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eee1ea5",
   "metadata": {},
   "source": [
    "Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c09a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0.0, std=0.05)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.layer1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.layer3(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        out = self.output(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a695c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_dims=784, hidden_dims=256, output_dims=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4de8cc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train_Loss: 1.4149, Train_Acc: 0.6175\n",
      "Epoch 2/10, Train_Loss: 0.3008, Train_Acc: 0.9136\n",
      "Epoch 3/10, Train_Loss: 0.2044, Train_Acc: 0.9404\n",
      "Epoch 4/10, Train_Loss: 0.1498, Train_Acc: 0.9562\n",
      "Epoch 5/10, Train_Loss: 0.1176, Train_Acc: 0.9657\n",
      "Epoch 6/10, Train_Loss: 0.0959, Train_Acc: 0.9722\n",
      "Epoch 7/10, Train_Loss: 0.0784, Train_Acc: 0.9759\n",
      "Epoch 8/10, Train_Loss: 0.0622, Train_Acc: 0.9815\n",
      "Epoch 9/10, Train_Loss: 0.0529, Train_Acc: 0.9835\n",
      "Epoch 10/10, Train_Loss: 0.0429, Train_Acc: 0.9871\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    t_loss = 0\n",
    "    t_acc = 0\n",
    "    cnt = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t_loss += loss.item()\n",
    "        t_acc += (torch.argmax(outputs, 1) == y).sum().item()\n",
    "        cnt += len(y)\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    t_acc /= cnt\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train_Loss: {t_loss:.4f}, Train_Acc: {t_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b5a76c",
   "metadata": {},
   "source": [
    "Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d73caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.layer1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.layer3(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        out = self.output(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8eccb5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_dims=784, hidden_dims=256, output_dims=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58082f6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train_Loss: 1.1696, Train_Acc: 0.6636\n",
      "Epoch 2/10, Train_Loss: 0.2465, Train_Acc: 0.9284\n",
      "Epoch 3/10, Train_Loss: 0.1610, Train_Acc: 0.9532\n",
      "Epoch 4/10, Train_Loss: 0.1173, Train_Acc: 0.9659\n",
      "Epoch 5/10, Train_Loss: 0.0893, Train_Acc: 0.9735\n",
      "Epoch 6/10, Train_Loss: 0.0726, Train_Acc: 0.9785\n",
      "Epoch 7/10, Train_Loss: 0.0576, Train_Acc: 0.9832\n",
      "Epoch 8/10, Train_Loss: 0.0464, Train_Acc: 0.9863\n",
      "Epoch 9/10, Train_Loss: 0.0364, Train_Acc: 0.9898\n",
      "Epoch 10/10, Train_Loss: 0.0289, Train_Acc: 0.9917\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    t_loss = 0\n",
    "    t_acc = 0\n",
    "    cnt = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t_loss += loss.item()\n",
    "        t_acc += (torch.argmax(outputs, 1) == y).sum().item()\n",
    "        cnt += len(y)\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    t_acc /= cnt\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train_Loss: {t_loss:.4f}, Train_Acc: {t_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d98e20",
   "metadata": {},
   "source": [
    "Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0dd3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dims)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        out = self.output(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "202978e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_dims=784, hidden_dims=256, output_dims=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29537ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train_Loss: 1.1274, Train_Acc: 0.6554\n",
      "Epoch 2/10, Train_Loss: 0.2558, Train_Acc: 0.9315\n",
      "Epoch 3/10, Train_Loss: 0.1504, Train_Acc: 0.9589\n",
      "Epoch 4/10, Train_Loss: 0.1028, Train_Acc: 0.9723\n",
      "Epoch 5/10, Train_Loss: 0.0721, Train_Acc: 0.9816\n",
      "Epoch 6/10, Train_Loss: 0.0520, Train_Acc: 0.9871\n",
      "Epoch 7/10, Train_Loss: 0.0365, Train_Acc: 0.9916\n",
      "Epoch 8/10, Train_Loss: 0.0243, Train_Acc: 0.9956\n",
      "Epoch 9/10, Train_Loss: 0.0178, Train_Acc: 0.9970\n",
      "Epoch 10/10, Train_Loss: 0.0127, Train_Acc: 0.9985\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    t_loss = 0\n",
    "    t_acc = 0\n",
    "    cnt = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t_loss += loss.item()\n",
    "        t_acc += (torch.argmax(outputs, 1) == y).sum().item()\n",
    "        cnt += len(y)\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    t_acc /= cnt\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train_Loss: {t_loss:.4f}, Train_Acc: {t_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879f1f4d",
   "metadata": {},
   "source": [
    "Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "286aba78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dims)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        identity = x # Skip connection\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = self.bn1(x)\n",
    "        x = nn.LeakyReLU()(x)\n",
    "        \n",
    "        x += identity  # Skip connection\n",
    "        out = self.output(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "035f62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(input_dims=784, hidden_dims=256, output_dims=10).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.RAdam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fe348af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train_Loss: 1.2211, Train_Acc: 0.6288\n",
      "Epoch 2/10, Train_Loss: 0.2635, Train_Acc: 0.9255\n",
      "Epoch 3/10, Train_Loss: 0.1601, Train_Acc: 0.9546\n",
      "Epoch 4/10, Train_Loss: 0.1103, Train_Acc: 0.9695\n",
      "Epoch 5/10, Train_Loss: 0.0803, Train_Acc: 0.9787\n",
      "Epoch 6/10, Train_Loss: 0.0588, Train_Acc: 0.9849\n",
      "Epoch 7/10, Train_Loss: 0.0424, Train_Acc: 0.9898\n",
      "Epoch 8/10, Train_Loss: 0.0303, Train_Acc: 0.9940\n",
      "Epoch 9/10, Train_Loss: 0.0220, Train_Acc: 0.9966\n",
      "Epoch 10/10, Train_Loss: 0.0154, Train_Acc: 0.9979\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    t_loss = 0\n",
    "    t_acc = 0\n",
    "    cnt = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        t_loss += loss.item()\n",
    "        t_acc += (torch.argmax(outputs, 1) == y).sum().item()\n",
    "        cnt += len(y)\n",
    "\n",
    "    t_loss /= len(train_loader)\n",
    "    t_acc /= cnt\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{num_epochs}, Train_Loss: {t_loss:.4f}, Train_Acc: {t_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52a0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aio2024-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
