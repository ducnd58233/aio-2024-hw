{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8 2.2 0.3 2.7]\n",
      " [1.9 1.1 0.9 2.4]\n",
      " [1.4 0.9 0.7 2. ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "word_embedding = np.array([\n",
    "    [0.8, 1.2, 0.3, 1.7],\n",
    "    [1.1, 0.6, 0.9, 1.4],\n",
    "    [0.5, 1.3, 0.7, 1.0]\n",
    "])\n",
    "\n",
    "def get_position_encoding(sequence_len, d_model):\n",
    "    # Create position array [0, 1, 2, ..., sequence_len-1]\n",
    "    position = np.arange(sequence_len)[:, np.newaxis]  # Shape: (seq_len, 1)\n",
    "    \n",
    "    # Create division term array [0, 2, 4, ..., d_model-2]\n",
    "    div_term = np.arange(0, d_model, 2)\n",
    "    \n",
    "    denominator = np.power(10000, div_term / d_model)\n",
    "    \n",
    "    pe = np.zeros((sequence_len, d_model))\n",
    "    \n",
    "    pe[:, 0::2] = np.sin(position / denominator)\n",
    "    pe[:, 1::2] = np.cos(position / denominator)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "sequence_len = word_embedding.shape[0]\n",
    "d_model = word_embedding.shape[1]\n",
    "\n",
    "positional_encoding = get_position_encoding(sequence_len, d_model)\n",
    "\n",
    "positional_embedding = word_embedding + positional_encoding\n",
    "print(np.round(positional_embedding, 1))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.  2.2 4.4 3.8]\n",
      " [2.6 3.3 4.3 4. ]\n",
      " [2.1 2.7 3.6 3.3]]\n",
      "[[3.6 5.1 4.3 4.3]\n",
      " [3.1 5.1 4.8 3.5]\n",
      " [2.5 4.1 3.9 2.8]]\n",
      "[[2.1 1.8 1.8 1.6]\n",
      " [1.7 1.7 2.  2.2]\n",
      " [1.4 1.5 1.7 1.9]]\n"
     ]
    }
   ],
   "source": [
    "query_weights = np.array([\n",
    "    [0.5, 0.3, 0.7, 0.2],\n",
    "    [0.8, 0.1, 0.5, 0.2],\n",
    "    [0.3, 0.8, 0.9, 0.5],\n",
    "    [0.5, 0.8, 0.9, 0.2],\n",
    "])\n",
    "query_bias = np.array([\n",
    "    [0.2, 0.7, 0.8, 0.8],\n",
    "    [0.2, 0.7, 0.8, 0.8],\n",
    "    [0.2, 0.7, 0.8, 0.8],\n",
    "])\n",
    "\n",
    "key_weights = np.array([\n",
    "    [0.2, 0.7, 0.2, 0.5],\n",
    "    [0.8, 0.8, 0.2, 0.8],\n",
    "    [0.5, 0.3, 0.8, 0.8],\n",
    "    [0.2, 0.9, 0.2, 0.7],\n",
    "])\n",
    "key_bias = np.array([\n",
    "    [0.5, 0.5, 0.8, 0.2],\n",
    "    [0.5, 0.5, 0.8, 0.2],\n",
    "    [0.5, 0.5, 0.8, 0.2],\n",
    "])\n",
    "\n",
    "value_weights = np.array([\n",
    "    [0.07, 0.33, 0.03, 0.37],\n",
    "    [0.12, 0.28, 0.18, 0.22],\n",
    "    [0.27, 0.13, 0.23, 0.17],\n",
    "    [0.38, 0.04, 0.32, 0.06],\n",
    "])\n",
    "value_bias = np.array([\n",
    "    [0.28, 0.47, 0.72, 0.95],\n",
    "    [0.28, 0.47, 0.72, 0.95],\n",
    "    [0.28, 0.47, 0.72, 0.95],\n",
    "])\n",
    "\n",
    "# Multi-head attention\n",
    "query_matrix = np.matmul(positional_embedding, query_weights.T) + query_bias\n",
    "key_matrix = np.matmul(positional_embedding, key_weights.T) + key_bias\n",
    "value_matrix = np.matmul(positional_embedding, value_weights.T) + value_bias\n",
    "\n",
    "print(np.round(query_matrix, 1))\n",
    "print(np.round(key_matrix, 1))\n",
    "print(np.round(value_matrix, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[26.9 25.9 21.2]\n",
      " [30.9 29.6 24.2]\n",
      " [25.2 24.1 19.8]]\n"
     ]
    }
   ],
   "source": [
    "scaled_dot_product_attention = np.matmul(query_matrix, key_matrix.T) / np.sqrt(d_model)\n",
    "scaled_dot_product_attention = np.round(scaled_dot_product_attention, 1)\n",
    "print(scaled_dot_product_attention)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.1 1.8 1.8 1.6]\n",
      " [2.  1.8 1.8 1.7]\n",
      " [1.9 1.8 1.8 1.8]]\n"
     ]
    }
   ],
   "source": [
    "scaled_dot_product = np.array([\n",
    "    [27.1, 26.0, 21.2],\n",
    "    [30.9, 29.6, 24.2],\n",
    "    [25.3, 24.7, 19.8]\n",
    "])\n",
    "\n",
    "mask = np.array([\n",
    "    [0, -float('inf'), -float('inf')],\n",
    "    [0, 0, -float('inf')],\n",
    "    [0, 0, 0]\n",
    "])\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "\n",
    "scaled_dot_product_attention = softmax(scaled_dot_product + mask)\n",
    "attention_output = np.matmul(scaled_dot_product_attention, value_matrix)\n",
    "print(np.round(attention_output, 1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48  0.77 -1.39  1.11]\n",
      " [ 0.82 -0.82 -1.15  1.15]\n",
      " [ 0.44 -0.73 -1.12  1.42]]\n"
     ]
    }
   ],
   "source": [
    "# attention_output = np.array([\n",
    "#     [2.1, 1.8, 1.8, 1.6],\n",
    "#     [2.0, 1.8, 1.8, 1.7],\n",
    "#     [1.9, 1.8, 1.8, 1.8]\n",
    "# ])\n",
    "\n",
    "# positional_embedding = np.array([\n",
    "#     [0.8, 2.2, 0.3, 2.7],\n",
    "#     [1.9, 1.1, 0.9, 2.4],\n",
    "#     [1.4, 0.9, 0.7, 2.0]\n",
    "# ])\n",
    "\n",
    "def layer_norm(x: np.ndarray, gamma: float = 1.0, beta: float = 0.0, epsilon: float = 0.0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies Layer Normalization to the input tensor.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor\n",
    "        gamma: Scaling parameter (default = 1.0)\n",
    "        beta: Shift parameter (default = 0.0)\n",
    "        epsilon: Small constant for numerical stability (default = 0.0)\n",
    "    \n",
    "    Returns:\n",
    "        Normalized tensor\n",
    "    \"\"\"\n",
    "    # Calculate mean along the last axis\n",
    "    mean = np.mean(x, axis=-1, keepdims=True)\n",
    "\n",
    "    # Calculate variance along the last axis\n",
    "    var = np.var(x, axis=-1, keepdims=True)\n",
    "\n",
    "    # Normalize\n",
    "    normalized = (x - mean) / np.sqrt(var + epsilon)\n",
    "\n",
    "    # Scale and shift\n",
    "    return gamma * normalized + beta\n",
    "\n",
    "combined = np.round(attention_output, 1) + np.round(positional_embedding, 1)\n",
    "layer_norm_output = layer_norm(combined, gamma=1.0, beta=0.0, epsilon=0.0)\n",
    "\n",
    "print(np.round(layer_norm_output, 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
