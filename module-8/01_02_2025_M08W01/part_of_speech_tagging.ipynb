{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![POS Tagging Pipeline](./assets/images/pos_tagging_pipeline.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 3914\n",
      "Number of sentences: 3914\n",
      "Sample sentence:  ['pierre', 'vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '29', '.']\n",
      "Sample tags:  ['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NNP', 'CD', '.']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "print(f'Number of samples: {len(tagged_sentences)}')\n",
    "\n",
    "sentences, sentence_tags = [], []\n",
    "for tagged_sentence in tagged_sentences:\n",
    "    sentence, tags = zip(*tagged_sentence)\n",
    "    sentences.append([word.lower() for word in sentence])\n",
    "    sentence_tags.append(list(tags))\n",
    "\n",
    "print(f'Number of sentences: {len(sentences)}')\n",
    "print('Sample sentence: ', sentences[0])\n",
    "print('Sample tags: ', sentence_tags[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2739\n",
      "Number of test samples: 588\n",
      "Number of validation samples: 587\n"
     ]
    }
   ],
   "source": [
    "train_sentences, test_sentences, train_tags, test_tags = train_test_split(\n",
    "    sentences, sentence_tags, test_size=0.3, random_state=42\n",
    ")\n",
    "val_sentences, test_sentences, val_tags, test_tags = train_test_split(\n",
    "    test_sentences, test_tags, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f'Number of training samples: {len(train_sentences)}')\n",
    "print(f'Number of test samples: {len(test_sentences)}')\n",
    "print(f'Number of validation samples: {len(val_sentences)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize sentences\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "MAX_LEN = 256\n",
    "PAD_TOKEN = '[PAD]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POSTaggingDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        sentences: List[List[str]], \n",
    "        tags: List[List[str]], \n",
    "        tokenizer: AutoTokenizer, \n",
    "        label2id: Dict[str, int],\n",
    "        max_len: int = MAX_LEN,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.label2id = label2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tags = self.tags[idx]\n",
    "\n",
    "        input_token = self.tokenizer.convert_tokens_to_ids(sentence)\n",
    "        attention_mask = [1] * len(input_token)\n",
    "        labels = [self.label2id[tag] for tag in tags]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': self.pad_and_truncate(input_token, pad_id=self.tokenizer.pad_token_id),\n",
    "            'labels': self.pad_and_truncate(labels, pad_id=self.label2id[PAD_TOKEN]),\n",
    "            'attention_mask': self.pad_and_truncate(attention_mask, pad_id=0),\n",
    "        }\n",
    "        \n",
    "    def pad_and_truncate(self, input_token: List[int], pad_id: int):\n",
    "        if len(input_token) >= self.max_len:\n",
    "            input_token = input_token[:self.max_len]\n",
    "        else:\n",
    "            input_token.extend([pad_id] * (self.max_len - len(input_token)))\n",
    "        return torch.as_tensor(input_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "{'NNP': 0, 'MD': 1, '-NONE-': 2, 'VBP': 3, \"''\": 4, 'JJ': 5, 'LS': 6, 'PRP': 7, '#': 8, 'WDT': 9, '``': 10, 'DT': 11, 'POS': 12, 'NNS': 13, 'IN': 14, 'WP': 15, 'JJR': 16, 'NN': 17, 'VBN': 18, 'FW': 19, 'JJS': 20, 'PRP$': 21, 'VBD': 22, ':': 23, 'WP$': 24, '-LRB-': 25, 'TO': 26, 'WRB': 27, 'NNPS': 28, 'VB': 29, 'EX': 30, 'CC': 31, 'RB': 32, 'RBS': 33, 'VBZ': 34, 'PDT': 35, '-RRB-': 36, ',': 37, 'UH': 38, 'RP': 39, '.': 40, 'RBR': 41, 'VBG': 42, '$': 43, 'SYM': 44, 'CD': 45, '[PAD]': 46}\n"
     ]
    }
   ],
   "source": [
    "unique_tags = set(tag for tags in sentence_tags for tag in tags)\n",
    "label2id = {tag: i for i, tag in enumerate(unique_tags)}\n",
    "label2id.update({PAD_TOKEN: len(label2id)})\n",
    "id2label = {i: tag for tag, i in label2id.items()}\n",
    "unique_tags.add(PAD_TOKEN)\n",
    "print(len(label2id))\n",
    "print(label2id)\n",
    "\n",
    "train_dataset = POSTaggingDataset(\n",
    "    train_sentences, train_tags, tokenizer, label2id\n",
    ")\n",
    "test_dataset = POSTaggingDataset(\n",
    "    test_sentences, test_tags, tokenizer, label2id\n",
    ")\n",
    "val_dataset = POSTaggingDataset(\n",
    "    val_sentences, val_tags, tokenizer, label2id\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at QCRI/bert-base-multilingual-cased-pos-english and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([46, 768]) in the checkpoint and torch.Size([47, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([46]) in the checkpoint and torch.Size([47]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model_name = 'QCRI/bert-base-multilingual-cased-pos-english'\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label2id),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "accuracy = evaluate.load('accuracy')\n",
    "ignore_label = len(label2id)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds, labels = p\n",
    "    mask = labels != ignore_label\n",
    "    preds = np.argmax(preds, axis=-1)\n",
    "    return accuracy.compute(\n",
    "        predictions=preds[mask], \n",
    "        references=labels[mask], \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1720' max='1720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1720/1720 6:39:20, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.117900</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.975664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.069400</td>\n",
       "      <td>0.057556</td>\n",
       "      <td>0.984581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.051700</td>\n",
       "      <td>0.045646</td>\n",
       "      <td>0.987975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.041235</td>\n",
       "      <td>0.989007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.039900</td>\n",
       "      <td>0.038206</td>\n",
       "      <td>0.989539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.036355</td>\n",
       "      <td>0.990118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.035458</td>\n",
       "      <td>0.990411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.034829</td>\n",
       "      <td>0.990484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.034378</td>\n",
       "      <td>0.990537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.030900</td>\n",
       "      <td>0.034320</td>\n",
       "      <td>0.990604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1720, training_loss=0.07758897974394088, metrics={'train_runtime': 23981.0756, 'train_samples_per_second': 1.142, 'train_steps_per_second': 0.072, 'total_flos': 3579914951838720.0, 'train_loss': 0.07758897974394088, 'epoch': 10.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./models',\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PRP VBP VBG DT NN IN JJ NN '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenization\n",
    "test_sentence = \"We are exploring the topic of deep learning\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "input = torch.as_tensor([tokenizer.convert_tokens_to_ids(test_sentence.split())])\n",
    "input = input.to(device)\n",
    "\n",
    "# prediction\n",
    "outputs = model(input)\n",
    "_, preds = torch.max(outputs.logits,-1)\n",
    "preds = preds[0].cpu().numpy()\n",
    "\n",
    "# decode\n",
    "pred_tags = \"\"\n",
    "for pred in preds:\n",
    "    pred_tags += id2label[pred] + \" \"\n",
    "\n",
    "pred_tags # => PRP VBP RB DT NN IN JJ NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
