{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\aio2024-hw\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n",
      "Path to dataset files: C:\\Users\\Admin\\.cache\\kagglehub\\datasets\\andrewmvd\\dog-and-cat-detection\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "data_dir = kagglehub.dataset_download(\n",
    "    'andrewmvd/dog-and-cat-detection',\n",
    ")\n",
    "print('Path to dataset files:', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from utils.memory_tracker import MemoryTracker, safe_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations_dir,\n",
    "        image_dir,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = self.filter_images_with_multiple_objects()\n",
    "        \n",
    "    def filter_images_with_multiple_objects(self):\n",
    "        valid_image_files = []\n",
    "        for f in os.listdir(self.image_dir):\n",
    "            if not os.path.isfile(os.path.join(self.image_dir, f)):\n",
    "                continue\n",
    "            img_name = f\n",
    "            annotation_name = os.path.splitext(img_name)[0] + '.xml'\n",
    "            annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "            \n",
    "            if self.count_objects_in_annotation(annotation_path) <= 1:\n",
    "                valid_image_files.append(img_name)\n",
    "            else:\n",
    "                print(f'Image {img_name} has multiple objects and will be excluded from the dataset')\n",
    "        return valid_image_files\n",
    "    \n",
    "    def count_objects_in_annotation(self, annotation_path):\n",
    "        try:\n",
    "            tree = ET.parse(annotation_path)\n",
    "            root = tree.getroot()\n",
    "            count = 0\n",
    "            for _ in root.findall('object'):\n",
    "                count += 1\n",
    "            return count\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "        \n",
    "    def parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        label = None\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            if not label:\n",
    "                label = name\n",
    "                \n",
    "        label_num = 0 if label == 'cat' else 1 if label == 'dog' else -1\n",
    "        return label_num\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        annotation_name = os.path.splitext(img_name)[0] + '.xml'\n",
    "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "        \n",
    "        label = self.parse_annotation(annotation_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cats_Test0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cats_Test1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cats_Test10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cats_Test100.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cats_Test1000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>Cats_Test995.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>Cats_Test996.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>Cats_Test997.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>Cats_Test998.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>Cats_Test999.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3686 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_name\n",
       "0        Cats_Test0.png\n",
       "1        Cats_Test1.png\n",
       "2       Cats_Test10.png\n",
       "3      Cats_Test100.png\n",
       "4     Cats_Test1000.png\n",
       "...                 ...\n",
       "3681   Cats_Test995.png\n",
       "3682   Cats_Test996.png\n",
       "3683   Cats_Test997.png\n",
       "3684   Cats_Test998.png\n",
       "3685   Cats_Test999.png\n",
       "\n",
       "[3686 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_dir = os.path.join(data_dir, 'annotations')\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "df = pd.DataFrame({'image_name': image_files})\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Cats_Test736.png has multiple objects and will be excluded from the dataset\n",
      "Image Cats_Test736.png has multiple objects and will be excluded from the dataset\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225],\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(annotations_dir, image_dir, transform)\n",
    "val_dataset = ImageDataset(annotations_dir, image_dir, transform)\n",
    "\n",
    "train_dataset.image_files = [f for f in train_dataset.image_files if f in train_df['image_name'].values]\n",
    "val_dataset.image_files = [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    scaler: torch.amp.GradScaler,\n",
    "    device: torch.device,\n",
    "    desc: str = 'Training',\n",
    "    position: int = 1,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = total_acc = total_count = 0\n",
    "    \n",
    "    try:        \n",
    "        with tqdm(\n",
    "            dataloader,\n",
    "            desc=desc,\n",
    "            unit='batch',\n",
    "            total=len(dataloader),\n",
    "            position=position,\n",
    "            leave=True,\n",
    "        ) as pbar:\n",
    "            for batch_idx, (data, targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    with autocast(device, dtype=torch.float16):\n",
    "                        scores = model(data)\n",
    "                        loss = criterion(scores, targets)\n",
    "                    \n",
    "                    # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "                    # Backward passes under autocast are not recommended.\n",
    "                    # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "                    scaler.scale(loss).backward()\n",
    "\n",
    "                    # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "                    # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "                    # otherwise, optimizer.step() is skipped.\n",
    "                    scaler.step(optimizer)\n",
    "\n",
    "                    # Updates the scale for next iteration.\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    total_loss += float(loss.detach().item() * targets.size(0))\n",
    "                    _, predictions = scores.max(1)\n",
    "                    total_acc += (predictions == targets).sum().item()\n",
    "                    total_count += data.size(0)\n",
    "                    \n",
    "                    del data, targets, scores, predictions\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"\\nError in training batch {batch_idx}: {str(e)}\")\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "                \n",
    "                allocated, reserved = MemoryTracker.get_memory_stats()\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{total_loss/max(1, total_count):.4f}',\n",
    "                    'Acc': f'{100.*total_acc/max(1, total_count):.4f}%',\n",
    "                    'Allocated GPU': f'{allocated:.2f}MB',\n",
    "                    'Reserved GPU': f'{reserved:.2f}MB'\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining error: {str(e)}\")\n",
    "        MemoryTracker.clear_memory(model)\n",
    "        raise\n",
    "\n",
    "    epoch_loss = total_loss / max(1, total_count)\n",
    "    epoch_acc = total_acc / max(1, total_count)\n",
    "    \n",
    "    return epoch_acc, epoch_loss\n",
    "\n",
    "def eval(\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    desc: str = 'Validating',\n",
    "    position: int = 1,\n",
    "    is_leaving = False\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss = total_acc = total_count = 0\n",
    "    \n",
    "    try:  \n",
    "        with tqdm(\n",
    "            dataloader,\n",
    "            desc=desc,\n",
    "            unit='sample',\n",
    "            unit_scale=dataloader.batch_size,\n",
    "            position=position,\n",
    "            leave=is_leaving,\n",
    "        ) as pbar:\n",
    "            for batch_idx, (data, targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    data = safe_to_device(data, device)\n",
    "                    targets = safe_to_device(targets, device)\n",
    "                    \n",
    "                    scores = model(data)\n",
    "                    loss = criterion(scores, targets)\n",
    "                    \n",
    "                    total_loss += float(loss.detach().item() * targets.size(0))\n",
    "                    _, predictions = scores.max(1)\n",
    "                    total_acc += (predictions == targets).sum().item()\n",
    "                    total_count += data.size(0)\n",
    "                    \n",
    "                    del data, targets, scores, predictions\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"\\nError in training batch {batch_idx}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "                allocated, reserved = MemoryTracker.get_memory_stats()\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{total_loss/max(1, total_count):.4f}',\n",
    "                    'Acc': f'{100.*total_acc/max(1, total_count):.4f}%',\n",
    "                    'Allocated GPU': f'{allocated:.2f}MB',\n",
    "                    'Reserved GPU': f'{reserved:.2f}MB'\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining error: {str(e)}\")\n",
    "        MemoryTracker.clear_memory(model)\n",
    "        raise\n",
    "\n",
    "    epoch_loss = total_loss / max(1, total_count)\n",
    "    epoch_acc = total_acc / max(1, total_count)\n",
    "    \n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: torch.nn.Module,\n",
    "    criterion: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int,\n",
    ") -> Dict[str, List[float]]:\n",
    "    history = {\n",
    "        'train_acc': [], 'train_loss': [],\n",
    "        'val_acc': [], 'val_loss': [],\n",
    "        'epoch_times': [], 'gpu_allocated': [], 'gpu_reserved': []\n",
    "    }\n",
    "    try:\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        with tqdm(range(epochs), desc=\"Epochs\", position=0, leave=True) as epoch_pbar:\n",
    "            for epoch in epoch_pbar:\n",
    "                try:\n",
    "                    epoch_start = time.time()\n",
    "\n",
    "                    train_acc, train_loss = train(\n",
    "                        model, optimizer, criterion, train_loader, scaler, device,\n",
    "                        desc=f\"Epoch {epoch+1}/{epochs} [Train]\",\n",
    "                        position=0,\n",
    "                    )\n",
    "\n",
    "                    val_acc, val_loss = eval(\n",
    "                        model, criterion, val_loader, device,\n",
    "                        desc=f\"Epoch {epoch+1}/{epochs} [Val]\",\n",
    "                        position=0,\n",
    "                        is_leaving=True,\n",
    "                    )\n",
    "\n",
    "                    epoch_time = time.time() - epoch_start\n",
    "                    allocated, reserved = MemoryTracker.get_memory_stats()\n",
    "\n",
    "                    history['train_acc'].append(train_acc)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "                    history['val_acc'].append(val_acc)\n",
    "                    history['val_loss'].append(val_loss)\n",
    "                    history['epoch_times'].append(epoch_time)\n",
    "                    history['gpu_allocated'].append(allocated)\n",
    "                    history['gpu_reserved'].append(reserved)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError in epoch {epoch + 1}: {str(e)}\")\n",
    "                    MemoryTracker.clear_memory(model)\n",
    "                    continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining loop error: {str(e)}\")\n",
    "        MemoryTracker.clear_memory(model)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\aio2024-hw\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/93 [00:00<?, ?batch/s]c:\\Users\\Admin\\anaconda3\\envs\\aio2024-hw\\Lib\\site-packages\\torch\\amp\\autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Epoch 1/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [05:21<00:00,  3.46s/batch, Loss=0.2405, Acc=90.2952%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 1/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:35<00:00, 21.39sample/s, Loss=0.2033, Acc=90.9214%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 2/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [05:45<00:00,  3.71s/batch, Loss=0.1229, Acc=94.9101%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 2/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:37<00:00, 20.52sample/s, Loss=0.3813, Acc=82.2493%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 3/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [05:49<00:00,  3.76s/batch, Loss=0.1114, Acc=96.2335%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 3/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:37<00:00, 20.69sample/s, Loss=0.1943, Acc=91.4634%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 4/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [05:25<00:00,  3.50s/batch, Loss=0.1390, Acc=94.7065%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 4/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [00:36<00:00, 20.91sample/s, Loss=0.2090, Acc=90.9214%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 5/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [06:19<00:00,  4.08s/batch, Loss=0.0668, Acc=97.7604%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 5/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [01:00<00:00, 12.62sample/s, Loss=0.2155, Acc=92.8184%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 6/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [08:59<00:00,  5.81s/batch, Loss=0.0866, Acc=96.8782%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 6/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [01:00<00:00, 12.60sample/s, Loss=0.3018, Acc=89.5664%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 7/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [09:20<00:00,  6.02s/batch, Loss=0.2027, Acc=92.2973%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 7/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [01:02<00:00, 12.23sample/s, Loss=0.1612, Acc=93.3604%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 8/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [09:17<00:00,  6.00s/batch, Loss=0.0675, Acc=97.4211%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 8/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [01:02<00:00, 12.34sample/s, Loss=0.2712, Acc=90.1084%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 9/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [09:10<00:00,  5.92s/batch, Loss=0.0347, Acc=99.0159%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 9/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [01:00<00:00, 12.73sample/s, Loss=0.1650, Acc=94.7154%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 10/10 [Train]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 93/93 [09:15<00:00,  5.98s/batch, Loss=0.0240, Acc=99.2535%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epoch 10/10 [Val]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 768/768 [01:02<00:00, 12.31sample/s, Loss=0.2070, Acc=94.0379%, Allocated GPU=0.00MB, Reserved GPU=0.00MB]\n",
      "Epochs: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [1:23:22<00:00, 500.23s/it]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2) # 2 classes: cat and dog\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = safe_to_device(model, device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)\n",
    "\n",
    "history = fit(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    num_epochs,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak GPU memory usage: 0.00 MB\n"
     ]
    }
   ],
   "source": [
    "max_memory = max(history['gpu_allocated'])\n",
    "print(f\"Peak GPU memory usage: {max_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
