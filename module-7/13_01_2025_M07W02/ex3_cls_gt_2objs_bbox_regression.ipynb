{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\aio2024-hw\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n",
      "Path to dataset files: C:\\Users\\Admin\\.cache\\kagglehub\\datasets\\andrewmvd\\dog-and-cat-detection\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "data_dir = kagglehub.dataset_download(\n",
    "    'andrewmvd/dog-and-cat-detection',\n",
    ")\n",
    "print('Path to dataset files:', data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = False\n",
    "cudnn.deterministic = True\n",
    "cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.resnet import ResNet18_Weights, ResNet50_Weights\n",
    "from torch.cuda.amp import GradScaler, autocast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "from utils.memory_tracker import MemoryTracker, safe_to_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotations_dir,\n",
    "        image_dir,\n",
    "        transform=None,\n",
    "    ):\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = self.filter_images_with_multiple_objects()\n",
    "        \n",
    "    def filter_images_with_multiple_objects(self):\n",
    "        valid_image_files = []\n",
    "        for f in os.listdir(self.image_dir):\n",
    "            if not os.path.isfile(os.path.join(self.image_dir, f)):\n",
    "                continue\n",
    "            img_name = f\n",
    "            annotation_name = os.path.splitext(img_name)[0] + '.xml'\n",
    "            annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "            \n",
    "            if self.count_objects_in_annotation(annotation_path) == 1:\n",
    "                valid_image_files.append(img_name)\n",
    "            else:\n",
    "                print(f'Image {img_name} has multiple objects and will be excluded from the dataset')\n",
    "        return valid_image_files\n",
    "    \n",
    "    def count_objects_in_annotation(self, annotation_path):\n",
    "        try:\n",
    "            tree = ET.parse(annotation_path)\n",
    "            root = tree.getroot()\n",
    "            count = 0\n",
    "            for _ in root.findall('object'):\n",
    "                count += 1\n",
    "            return count\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "        \n",
    "    def parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        img_width = int(root.find('size/width').text)\n",
    "        img_height = int(root.find('size/height').text)\n",
    "        \n",
    "        label = None\n",
    "        bbox = None\n",
    "        for obj in root.findall('object'):\n",
    "            name = obj.find('name').text\n",
    "            if not label:\n",
    "                label = name\n",
    "                xmin = int(obj.find('bndbox/xmin').text)\n",
    "                ymin = int(obj.find('bndbox/ymin').text)\n",
    "                xmax = int(obj.find('bndbox/xmax').text)\n",
    "                ymax = int(obj.find('bndbox/ymax').text)\n",
    "                \n",
    "                bbox = [\n",
    "                    xmin / img_width,\n",
    "                    ymin / img_height,\n",
    "                    xmax / img_width,\n",
    "                    ymax / img_height,\n",
    "                ]\n",
    "                \n",
    "        label_num = 0 if label == 'cat' else 1 if label == 'dog' else -1\n",
    "        return label_num, torch.tensor(bbox, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1_file = self.image_files[idx]\n",
    "        img1_path = os.path.join(self.image_dir, img1_file)\n",
    "\n",
    "        annotation_name = os.path.splitext(img1_file)[0] + \".xml\"\n",
    "        img1_annotations = self.parse_annotation(\n",
    "            os.path.join(self.annotations_dir, annotation_name)\n",
    "        )\n",
    "\n",
    "        if idx == len(self.image_files) - 1:\n",
    "            idx2 = 0\n",
    "        else:\n",
    "            idx2 = idx + 1\n",
    "        img2_file = self.image_files[idx2]\n",
    "        img2_path = os.path.join(self.image_dir, img2_file)\n",
    "\n",
    "        annotation_name = os.path.splitext(img2_file)[0] + \".xml\"\n",
    "        img2_annotations = self.parse_annotation(\n",
    "            os.path.join(self.annotations_dir, annotation_name)\n",
    "        )\n",
    "\n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img2_path).convert(\"RGB\")\n",
    "\n",
    "        # Horizontal merge\n",
    "        merged_image = Image.new(\n",
    "            \"RGB\", (img1.width + img2.width, max(img1.height, img2.height))\n",
    "        )\n",
    "        merged_image.paste(img1, (0, 0))\n",
    "        merged_image.paste(img2, (img1.width, 0))\n",
    "        merged_w = img1.width + img2.width\n",
    "        merged_h = max(img1.height, img2.height)\n",
    "\n",
    "        merged_annotations = []\n",
    "\n",
    "        # Adjust bbox coordinates for objects from img1 AND normalize\n",
    "        new_bbox1 = [\n",
    "            img1_annotations[1][0] * img1.width / merged_w,  # Normalize xmin\n",
    "            img1_annotations[1][1] * img1.height / merged_h,  # Normalize ymin\n",
    "            img1_annotations[1][2] * img1.width / merged_w,  # Normalize xmax\n",
    "            img1_annotations[1][3] * img1.height / merged_h,  # Normalize ymax\n",
    "        ]\n",
    "        merged_annotations.append({\"bbox\": new_bbox1, \"label\": img1_annotations[0]})\n",
    "\n",
    "        # Adjust bbox coordinates for objects from img2 AND normalize\n",
    "        new_bbox2 = [\n",
    "            (img2_annotations[1][0] * img2.width + img1.width)\n",
    "            / merged_w,  # Normalize xmin\n",
    "            img2_annotations[1][1] * img2.height / merged_h,  # Normalize ymin\n",
    "            (img2_annotations[1][2] * img2.width + img1.width)\n",
    "            / merged_w,  # Normalize xmax\n",
    "            img2_annotations[1][3] * img2.height / merged_h,  # Normalize ymax\n",
    "        ]\n",
    "\n",
    "        merged_annotations.append({\"bbox\": new_bbox2, \"label\": img2_annotations[0]})\n",
    "\n",
    "        # Convert merged image to tensor\n",
    "        if self.transform:\n",
    "            merged_image = self.transform(merged_image)\n",
    "        else:\n",
    "            merged_image = transforms.ToTensor()(merged_image)\n",
    "\n",
    "        # Convert annotations to 1D tensors, with shape (4,) for bbox and (1,) for label\n",
    "        annotations = torch.zeros((len(merged_annotations), 5))\n",
    "        for i, ann in enumerate(merged_annotations):\n",
    "            annotations[i] = torch.cat(\n",
    "                (torch.tensor(ann[\"bbox\"]), torch.tensor([ann[\"label\"]]))\n",
    "            )\n",
    "\n",
    "        return merged_image, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cats_Test0.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cats_Test1.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cats_Test10.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cats_Test100.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cats_Test1000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3681</th>\n",
       "      <td>Cats_Test995.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3682</th>\n",
       "      <td>Cats_Test996.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3683</th>\n",
       "      <td>Cats_Test997.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3684</th>\n",
       "      <td>Cats_Test998.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3685</th>\n",
       "      <td>Cats_Test999.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3686 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             image_name\n",
       "0        Cats_Test0.png\n",
       "1        Cats_Test1.png\n",
       "2       Cats_Test10.png\n",
       "3      Cats_Test100.png\n",
       "4     Cats_Test1000.png\n",
       "...                 ...\n",
       "3681   Cats_Test995.png\n",
       "3682   Cats_Test996.png\n",
       "3683   Cats_Test997.png\n",
       "3684   Cats_Test998.png\n",
       "3685   Cats_Test999.png\n",
       "\n",
       "[3686 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_dir = os.path.join(data_dir, 'annotations')\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "df = pd.DataFrame({'image_name': image_files})\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Cats_Test736.png has multiple objects and will be excluded from the dataset\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "base_dataset = ImageDataset(annotations_dir, image_dir, transform)\n",
    "# Get the total dataset size\n",
    "dataset_size = len(base_dataset)\n",
    "\n",
    "# Calculate train and validation sizes\n",
    "val_size = int(0.2 * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "\n",
    "# Generate indices\n",
    "indices = np.arange(dataset_size)\n",
    "np.random.seed(42)  # Ensure reproducibility\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Split indices for train and validation sets\n",
    "train_indices, val_indices = indices[:train_size], indices[train_size:]\n",
    "# train_indices, val_indices = indices[:1], indices[:1]\n",
    "\n",
    "# Create Subsets using the appropriate base dataset\n",
    "train_dataset = Subset(base_dataset, train_indices)\n",
    "val_dataset = Subset(base_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    num_workers=4,\n",
    "    persistent_workers=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "num_classes = 2\n",
    "class_to_idx = {\"cat\": 0, \"dog\": 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleYOLO(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLO, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Remove the final classification layer of ResNet\n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "\n",
    "        # Add the YOLO head\n",
    "        self.fcs = nn.Linear(\n",
    "            2048, 2 * 2 * (4 + self.num_classes)\n",
    "        )  # 2 is for the number of grid cell\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, C, H, W)\n",
    "        features = self.backbone(x)\n",
    "        features = F.adaptive_avg_pool2d(\n",
    "            features, (1, 1)\n",
    "        )  # shape: (batch_size, 2048, 1, 1)\n",
    "        features = features.view(features.size(0), -1)  # shape: (batch_size, 2048)\n",
    "        features = self.fcs(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(output, targets, device, num_classes):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    batch_size = output.shape[0]\n",
    "    total_loss = 0\n",
    "\n",
    "    for i in range(batch_size):  # Iterate through each image in the batch\n",
    "        grid_have_object = torch.zeros((batch_size, 2, 2), device=device)\n",
    "\n",
    "        for j in range(len(targets[i])):  # Iterate through objects in the image\n",
    "            # Determine which grid cell the object's center falls into\n",
    "            # Assuming bbox coordinates are normalized to [0, 1]\n",
    "            bbox_center_x = (targets[i][j][0] + targets[i][j][2]) / 2\n",
    "            bbox_center_y = (targets[i][j][1] + targets[i][j][3]) / 2\n",
    "\n",
    "            grid_x = int(\n",
    "                bbox_center_x * 2\n",
    "            )  # Multiply by number of grid cells (2 in this case)\n",
    "            grid_y = int(bbox_center_y * 2)\n",
    "\n",
    "            grid_have_object[i, grid_y, grid_x] = 1\n",
    "\n",
    "            # 1. Classification Loss for the responsible grid cell\n",
    "            # Convert label to one-hot encoding only for this example\n",
    "            label_one_hot = torch.zeros(num_classes, device=device)\n",
    "            label_one_hot[int(targets[i][j][4])] = 1\n",
    "\n",
    "            # Classification loss (using CrossEntropyLoss)\n",
    "            classification_loss = ce_loss(output[i, grid_y, grid_x, 4:], label_one_hot)\n",
    "\n",
    "            # 2. Regression Loss for the responsible grid cell\n",
    "            bbox_target = targets[i][j][:4].to(device)\n",
    "            regression_loss = mse_loss(output[i, grid_y, grid_x, :4], bbox_target)\n",
    "\n",
    "            # import pdb; pdb.set_trace()\n",
    "\n",
    "            total_loss += classification_loss + regression_loss\n",
    "\n",
    "        # 3. No Object Loss (for other grid cells)\n",
    "        no_obj_loss = 0\n",
    "        for other_grid_y in range(2):\n",
    "            for other_grid_x in range(2):\n",
    "                if grid_have_object[i, other_grid_y, other_grid_x] == 0:\n",
    "                    # MSE loss for predicting no object (all zeros)\n",
    "                    no_obj_loss += mse_loss(\n",
    "                        output[i, other_grid_y, other_grid_x, :4],\n",
    "                        torch.zeros(4, device=device),\n",
    "                    )\n",
    "\n",
    "        total_loss += no_obj_loss\n",
    "\n",
    "    return total_loss / batch_size  # Average loss over the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    # Calculate intersection coordinates\n",
    "    x_a = max(box1[0], box2[0])\n",
    "    y_a = max(box1[1], box2[1])\n",
    "    x_b = min(box1[2], box2[2])\n",
    "    y_b = min(box1[3], box2[3])\n",
    "\n",
    "    # Compute the area of intersection\n",
    "    inter_area = max(0, x_b - x_a) * max(0, y_b - y_a)\n",
    "\n",
    "    # Compute the area of both the prediction and ground-truth\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    # Compute the IoU\n",
    "    iou = inter_area / float(box1_area + box2_area - inter_area)\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Dict, List\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    num_classes: int,\n",
    "    scaler: torch.amp.GradScaler,\n",
    "    device: torch.device,\n",
    "    desc: str = 'Training',\n",
    "    position: int = 1,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss = total_acc = total_count = 0\n",
    "    \n",
    "    try:        \n",
    "        with tqdm(\n",
    "            dataloader,\n",
    "            desc=desc,\n",
    "            unit='batch',\n",
    "            total=len(dataloader),\n",
    "            position=position,\n",
    "            leave=True,\n",
    "        ) as pbar:\n",
    "            for batch_idx, (images, targets) in enumerate(pbar):\n",
    "                try:\n",
    "                    optimizer.zero_grad()\n",
    "                    batch_size = images.shape[0]\n",
    "                    \n",
    "                    with autocast(device, dtype=torch.float16):\n",
    "                        output = model(images)\n",
    "                        output = output.view(batch_size)\n",
    "\n",
    "                        loss =  calculate_loss(output, targets, device, num_classes)\n",
    "                    \n",
    "                    # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
    "                    # Backward passes under autocast are not recommended.\n",
    "                    # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
    "                    scaler.scale(loss).backward()\n",
    "\n",
    "                    # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
    "                    # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
    "                    # otherwise, optimizer.step() is skipped.\n",
    "                    scaler.step(optimizer)\n",
    "\n",
    "                    # Updates the scale for next iteration.\n",
    "                    scaler.update()\n",
    "                    \n",
    "                    total_loss += float(loss.detach().item())\n",
    "                    _, predictions = output.max(1)\n",
    "                    total_acc += (predictions == targets).sum().item()\n",
    "                    total_count += output.size(0)\n",
    "                    \n",
    "                    del images, targets, output, predictions\n",
    "                except RuntimeError as e:\n",
    "                    print(f\"\\nError in training batch {batch_idx}: {str(e)}\")\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "                \n",
    "                allocated, reserved = MemoryTracker.get_memory_stats()\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f'{total_loss/max(1, total_count):.4f}',\n",
    "                    'Acc': f'{100.*total_acc/max(1, total_count):.4f}%',\n",
    "                    'Allocated GPU': f'{allocated:.2f}MB',\n",
    "                    'Reserved GPU': f'{reserved:.2f}MB'\n",
    "                })\n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining error: {str(e)}\")\n",
    "        MemoryTracker.clear_memory(model)\n",
    "        raise\n",
    "\n",
    "    epoch_loss = total_loss / max(1, total_count)\n",
    "    epoch_acc = total_acc / max(1, total_count)\n",
    "    \n",
    "    return epoch_acc, epoch_loss\n",
    "\n",
    "def eval(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    num_classes: int = 2,\n",
    "    desc: str = 'Validating',\n",
    "    position: int = 1,\n",
    "    is_leaving = False\n",
    ") -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    try:  \n",
    "        with tqdm(\n",
    "            dataloader,\n",
    "            desc=desc,\n",
    "            unit='sample',\n",
    "            unit_scale=dataloader.batch_size,\n",
    "            position=position,\n",
    "            leave=is_leaving,\n",
    "        ) as pbar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (images, targets) in enumerate(pbar):\n",
    "                    try:\n",
    "                        images = safe_to_device(images, device)\n",
    "                        \n",
    "                        output = model(images)\n",
    "                        # Reshape output to (batch_size, grid_y, grid_x, 4 + num_classes)\n",
    "                        output = output.view(images.shape[0], 2, 2, 4 + num_classes)\n",
    "                        \n",
    "                        total_loss = calculate_loss(output, targets, device, num_classes)\n",
    "                        running_loss += total_loss.item()\n",
    "                        \n",
    "                        # Process predictions and targets for mAP calculation\n",
    "                        for batch_idx in range(images.shape[0]):\n",
    "                            preds = []\n",
    "                            targs = []\n",
    "                            \n",
    "                            # Process ground truth targets\n",
    "                            for target in targets[batch_idx]:\n",
    "                                targs.append({\n",
    "                                    \"boxes\": target[:4].unsqueeze(0),\n",
    "                                    \"labels\": target[4].unsqueeze(0).long(),\n",
    "                                })\n",
    "                            \n",
    "                            # Match predictions to ground truth targets\n",
    "                            for target_idx, target in enumerate(targets[batch_idx]):\n",
    "                                best_iou = -1\n",
    "                                best_pred_idx = -1\n",
    "                                \n",
    "                                for grid_y in range(2):\n",
    "                                    for grid_x in range(2):\n",
    "                                        bbox_pred = output[batch_idx, grid_y, grid_x, :4]\n",
    "                                        \n",
    "                                        if torch.all(torch.round(bbox_pred, decimals=2) == 0):\n",
    "                                            continue\n",
    "                                            \n",
    "                                        iou = calculate_iou(bbox_pred, target[:4])\n",
    "                                        \n",
    "                                        if iou > best_iou:\n",
    "                                            best_iou = iou\n",
    "                                            best_pred_idx = (grid_y, grid_x)\n",
    "                                \n",
    "                                if best_pred_idx != -1:\n",
    "                                    grid_y, grid_x = best_pred_idx\n",
    "                                    bbox_pred = output[batch_idx, grid_y, grid_x, :4]\n",
    "                                    class_probs = torch.softmax(output[batch_idx, grid_y, grid_x, 4:], dim=0)\n",
    "                                    class_pred = class_probs.argmax().item()\n",
    "                                    confidence = class_probs[class_pred].item()\n",
    "                                    \n",
    "                                    preds.append({\n",
    "                                        \"boxes\": bbox_pred.unsqueeze(0),\n",
    "                                        \"scores\": torch.tensor([confidence], device=device),\n",
    "                                        \"labels\": torch.tensor([class_pred], device=device),\n",
    "                                    })\n",
    "                            \n",
    "                            # Add batch predictions and targets to overall lists\n",
    "                            if preds:\n",
    "                                all_preds.append({\n",
    "                                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                                    for k, v in preds[0].items()\n",
    "                                })\n",
    "                            \n",
    "                            if targs:\n",
    "                                all_targets.append({\n",
    "                                    k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                                    for k, v in targs[0].items()\n",
    "                                })\n",
    "                        \n",
    "                        allocated, reserved = MemoryTracker.get_memory_stats()\n",
    "                        pbar.set_postfix({\n",
    "                            'Loss': f'{running_loss/max(1, batch_idx+1):.4f}',\n",
    "                            'Allocated GPU': f'{allocated:.2f}MB',\n",
    "                            'Reserved GPU': f'{reserved:.2f}MB'\n",
    "                        })\n",
    "                        \n",
    "                    except RuntimeError as e:\n",
    "                        print(f\"\\nError in validation batch {batch_idx}: {str(e)}\")\n",
    "                        continue\n",
    "                        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nValidation error: {str(e)}\")\n",
    "        MemoryTracker.clear_memory(model)\n",
    "        raise\n",
    "\n",
    "    val_loss = running_loss / len(dataloader)\n",
    "    \n",
    "    # Calculate mAP\n",
    "    metric = MeanAveragePrecision()\n",
    "    metric.update(all_preds, all_targets)\n",
    "    mAP = metric.compute()[\"map\"]\n",
    "    \n",
    "    return mAP, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model: torch.nn.Module,\n",
    "    num_classes: int,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader,\n",
    "    val_loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    epochs: int,\n",
    ") -> Dict[str, List[float]]:\n",
    "    history = {\n",
    "        'train_acc': [], 'train_loss': [],\n",
    "        'val_acc': [], 'val_loss': [],\n",
    "        'epoch_times': [], 'gpu_allocated': [], 'gpu_reserved': []\n",
    "    }\n",
    "    try:\n",
    "        scaler = GradScaler()\n",
    "        \n",
    "        with tqdm(range(epochs), desc=\"Epochs\", position=0, leave=True) as epoch_pbar:\n",
    "            for epoch in epoch_pbar:\n",
    "                try:\n",
    "                    epoch_start = time.time()\n",
    "\n",
    "                    train_acc, train_loss = train(\n",
    "                        model, optimizer, train_loader, num_classes, scaler, device,\n",
    "                        desc=f\"Epoch {epoch+1}/{epochs} [Train]\",\n",
    "                        position=0,\n",
    "                    )\n",
    "\n",
    "                    val_acc, val_loss = eval(\n",
    "                        model, val_loader, device,\n",
    "                        num_classes,\n",
    "                        desc=f\"Epoch {epoch+1}/{epochs} [Val]\",\n",
    "                        position=0,\n",
    "                        is_leaving=True,\n",
    "                    )\n",
    "\n",
    "                    epoch_time = time.time() - epoch_start\n",
    "                    allocated, reserved = MemoryTracker.get_memory_stats()\n",
    "\n",
    "                    history['train_acc'].append(train_acc)\n",
    "                    history['train_loss'].append(train_loss)\n",
    "                    history['val_acc'].append(val_acc)\n",
    "                    history['val_loss'].append(val_loss)\n",
    "                    history['epoch_times'].append(epoch_time)\n",
    "                    history['gpu_allocated'].append(allocated)\n",
    "                    history['gpu_reserved'].append(reserved)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError in epoch {epoch + 1}: {str(e)}\")\n",
    "                    MemoryTracker.clear_memory(model)\n",
    "                    continue\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\nTraining loop error: {str(e)}\")\n",
    "        MemoryTracker.clear_memory(model)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\aio2024-hw\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "SimpleYOLO(\n",
      "  (backbone): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (5): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (6): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (7): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fcs): Linear(in_features=2048, out_features=24, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]:   0%|          | 0/368 [00:00<?, ?batch/s]"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "model = SimpleYOLO(num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = safe_to_device(model, device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-3)\n",
    "print(device)\n",
    "print(model)\n",
    "\n",
    "history = fit(\n",
    "    model,\n",
    "    num_classes,\n",
    "    optimizer,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    num_epochs,\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory = max(history['gpu_allocated'])\n",
    "print(f\"Peak GPU memory usage: {max_memory:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_samples(dataset, num_rows=3, num_cols=3, start_idx=0, class_to_idx=None):\n",
    "    \"\"\"\n",
    "    Visualize samples from dataset with bounding boxes and labels\n",
    "    \n",
    "    Args:\n",
    "        dataset: Dataset to visualize\n",
    "        num_rows: Number of rows in grid\n",
    "        num_cols: Number of columns in grid \n",
    "        start_idx: Starting index in dataset\n",
    "        class_to_idx: Dictionary mapping class names to indices\n",
    "    \"\"\"\n",
    "    # Add denormalization transform\n",
    "    denorm = transforms.Compose([\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "            std=[1/0.229, 1/0.224, 1/0.225]\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, ax in enumerate(axes):\n",
    "        merged_image, annotations = dataset[start_idx + idx]\n",
    "\n",
    "        # Denormalize the image before converting to PIL\n",
    "        if isinstance(merged_image, torch.Tensor):\n",
    "            merged_image = denorm(merged_image)\n",
    "            # Clamp values to [0,1] range\n",
    "            merged_image = torch.clamp(merged_image, 0, 1)\n",
    "            image = transforms.ToPILImage()(merged_image)\n",
    "        else:\n",
    "            image = merged_image\n",
    "\n",
    "        ax.imshow(image)\n",
    "\n",
    "        # Draw each bounding box on the image\n",
    "        for ann in annotations:\n",
    "            bbox = ann[:4]\n",
    "            label = int(ann[4].item())\n",
    "\n",
    "            # Scale bounding box coordinates to image size\n",
    "            width, height = image.size\n",
    "            x_min = bbox[0] * width\n",
    "            y_min = bbox[1] * height\n",
    "            x_max = bbox[2] * width\n",
    "            y_max = bbox[3] * height\n",
    "\n",
    "            # Create a rectangle patch\n",
    "            rect = patches.Rectangle(\n",
    "                (x_min, y_min),\n",
    "                x_max - x_min,\n",
    "                y_max - y_min,\n",
    "                linewidth=2,\n",
    "                edgecolor=\"g\",\n",
    "                facecolor=\"none\",\n",
    "            )\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "            # Add label text if class_to_idx is provided\n",
    "            if class_to_idx:\n",
    "                label_name = label\n",
    "                ax.text(\n",
    "                    x_min,\n",
    "                    y_min - 5,\n",
    "                    label_name,\n",
    "                    color=\"r\",\n",
    "                    fontsize=10,\n",
    "                    bbox=dict(facecolor=\"white\", alpha=0.7),\n",
    "                )\n",
    "\n",
    "        ax.set_title(f\"Sample {start_idx + idx}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot first grid (images 0-8)\n",
    "visualize_data_samples(val_dataset, start_idx=0, class_to_idx=class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aio2024-hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
